{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da7eaba-0251-48d7-87cd-d9980104b836",
   "metadata": {},
   "source": [
    "# Lab. 3-1 Schema Preparation-1\n",
    "\n",
    "This lab focuses on steps 1 and 2 of the Text2SQL process, specifically dealing with Schema Preparation.\n",
    "\n",
    "Schema Linking, which involves organizing the necessary schema for query generation, is highlighted as one of the most challenging aspects of Text2SQL for complex databases. It addresses real-world challenges in corporate environments, such as:\n",
    "1. Abbreviated or unclear table/column names\n",
    "2. Too many tables/columns to include in a single prompt\n",
    "\n",
    "To address this, we need to refine schema description documents tailored to our database and select the necessary context to provide to the LLM. In this notebook, we will simulate the schema preparation process using the Chinook DB description document. This is part of a larger workflow that will continue in subsequent labs\n",
    "\n",
    "![Intro](../images/text2sql/schema-prep-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa1a10-2fa1-4383-8838-3186b39815e8",
   "metadata": {},
   "source": [
    "## Step 0: OpenSearch Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "801cb0f5-8055-442d-814b-9a0cea8b9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q opensearch-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1c499-18e0-4fd7-a2d8-e89bb9b4e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.ssm import parameter_store\n",
    "\n",
    "pm = parameter_store('us-west-2')\n",
    "domain_endpoint = pm.get_params(key=\"chatbot-opensearch_domain_endpoint\", enc=False)\n",
    "opensearch_domain_endpoint = f\"https://{domain_endpoint}\"\n",
    "opensearch_user_id = pm.get_params(key=\"chatbot-opensearch_user_id\", enc=False)\n",
    "opensearch_user_password = pm.get_params(key=\"chatbot-opensearch_user_password\", enc=True)\n",
    "print(opensearch_domain_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385871de-60f4-4930-9ebb-a42f43985283",
   "metadata": {},
   "source": [
    "## Step 1: Load Schema Description Document (Corresponding to 1. Schema Loader in the above diagram)\n",
    "\n",
    "Companies may have schema description documents defined in formats like Excel or CSV. Let's assume we parse these and convert them into the following Schema Description format:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"table_name\": {\n",
    "        \"table_desc\": \"Description of the table\",\n",
    "        \"cols\": [\n",
    "            {\n",
    "                \"col\": \"Column Name 1\",\n",
    "                \"col_desc\": \"Description of the column including PK info\"\n",
    "            },\n",
    "            {\n",
    "                \"col\": \"Column Name 2\",\n",
    "                \"col_desc\": \"Description of the column\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The initial description document should include the table names, basic descriptions of the tables, column names, and descriptions of the columns. If a company doesn't have a well-organized schema description document, we could provide very basic information and have an LLM augment it to generate the initial description document itself. \n",
    "\n",
    "For the LLM call script to do this, refer to this [link](https://github.com/kevmyung/db-schema-loader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835783d0-5bc3-4bbd-bd16-9cf672270fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = './chinook_schema.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    schema_description = json.load(file)\n",
    "\n",
    "print(json.dumps(schema_description, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197834e-7ee3-4eb4-9398-d2e0d5789d53",
   "metadata": {},
   "source": [
    "#### Now we will continue with the follow-up tasks using the Schema Description document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca849b4-8126-4c9d-bafa-af3e7543fa44",
   "metadata": {},
   "source": [
    "## Step 2: Translating Sample SQL Queries to Natural Language Questions \n",
    "\n",
    "Providing good sample queries to the LLM is helpful not only for query writing but also for Schema Linking.\n",
    "\n",
    "However, since Text2SQL hasn't been used before, there are no natural language questions matched to existing SQL queries. Only the frequently used SQL queries are available.\n",
    "\n",
    "In Step 2, we will proceed with the SQL2Text process, converting these frequently used queries into natural language questions. (Corresponding to 3. Query Translator in the above diagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e40dcac-1073-4a4f-9c68-46944c2ce5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_file = './chinook_sample_queries.sql'\n",
    "\n",
    "with open(sql_file, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "queries = [query.strip() for query in data.split(';') if query.strip()]\n",
    "\n",
    "for i, query in enumerate(queries, start=1):\n",
    "    print(f\"Query {i}:\\n{query}\\n{'-'*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e58b4f-0b52-4eb1-9d8b-6121cf561aca",
   "metadata": {},
   "source": [
    "To interpret the queries, we extract the table/column information used in each query as follows:\n",
    "```\n",
    "{\n",
    "  \"table\": [\"table1\", \"table2\", ...],\n",
    "  \"column\": [\"col1\", \"col2\", ...]\n",
    "}\n",
    "```\n",
    "The following is the LLM request syntax for extracting the schema list used in SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c68a7-3c2c-402f-8a57-657d1e160251",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_sys_prompt = [{\n",
    "    \"text\": \"\"\" \n",
    "You are an expert in extracting table names and column names from SQL queries. \n",
    "From the provided SQL query, extract all table names and column names used for SELECT, WHERE, and JOIN clauses, excluding asterisks (\"*\"). \n",
    "Ensure that the response is in a valid JSON format that can be used directly with json.load(). \n",
    "Skip the preamble and only provide the answer in a JSON document:\n",
    "\n",
    "{\n",
    "  \"table\": [\"table1\", \"table2\", ...],\n",
    "  \"column\": [\"col1\", \"col2\", ...]\n",
    "}\n",
    "\n",
    "<input>\n",
    "SQL:\n",
    "SELECT * from sample_table \n",
    "where sample_column like '%something%'\n",
    "LIMIT 200;\n",
    "</input>\n",
    "\n",
    "<output>\n",
    "{\n",
    "  \"table\": [\"sample_table\"],\n",
    "  \"column\": [\"sample_column\"]\n",
    "}\n",
    "</output>\n",
    "\"\"\" \n",
    "}]\n",
    "\n",
    "\n",
    "def get_extraction_prompt(sql):\n",
    "    return [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": f\"SQL: \\n{sql}\"}]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c230b58-f709-4a7f-a5d9-78c6f7685021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "region_name = \"us-west-2\"\n",
    "llm_model = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "\n",
    "def init_boto3_client(region: str):\n",
    "    retry_config = Config(\n",
    "        region_name=region,\n",
    "        retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    "    )\n",
    "    return boto3.client(\"bedrock-runtime\", region_name=region, config=retry_config)\n",
    "\n",
    "def converse_with_bedrock(boto3_client, sys_prompt, usr_prompt):    \n",
    "    temperature = 0.0\n",
    "    top_p = 0.1\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "    \n",
    "    response = boto3_client.converse(\n",
    "        modelId=llm_model, \n",
    "        messages=usr_prompt, \n",
    "        system=sys_prompt,\n",
    "        inferenceConfig=inference_config\n",
    "    )\n",
    "\n",
    "    return response['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436d16f5-4d08-4f02-843f-dc487c620384",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_client = init_boto3_client(region_name)\n",
    "\n",
    "sql = queries[8].strip()\n",
    "\n",
    "response = converse_with_bedrock(boto3_client, extraction_sys_prompt, get_extraction_prompt(sql))\n",
    "used_schema = json.loads(response)\n",
    "print(used_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592db742-3638-4e33-93ea-2a9c31612142",
   "metadata": {},
   "source": [
    "For example, let's extract the schema used in the query below.\n",
    "\n",
    "```SELECT CustomerId, SUM(Total) AS TotalPurchase FROM Invoice GROUP BY CustomerId ORDER BY TotalPurchase DESC LIMIT 5``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2a722-bba8-4043-b627-8b48c0a45cfd",
   "metadata": {},
   "source": [
    "#### Now, let's look at the description of the schema used in this query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17816589-1b10-4200-844e-6d76f932c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_descriptions(table_info, tables, columns):\n",
    "    tables_lower = {table.lower() for table in tables}\n",
    "    columns_lower = {column.lower() for column in columns}\n",
    "    \n",
    "    description = {\n",
    "        \"table\": {},\n",
    "        \"column\": {}\n",
    "    }\n",
    "    \n",
    "    for table_schema in table_info:\n",
    "        for table_name, table_info in table_schema.items():\n",
    "            if table_name.lower() in tables_lower:\n",
    "                description[\"table\"][table_name] = table_info[\"table_desc\"]\n",
    "                for col in table_info[\"cols\"]:\n",
    "                    col_name = col[\"col\"]\n",
    "                    if col_name.lower() in columns_lower:\n",
    "                        description[\"column\"][col_name] = col[\"col_desc\"]\n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052c9d03-949a-45b7-9b0d-3de07601961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_description = extract_descriptions(schema_description, used_schema['table'], used_schema['column'])\n",
    "print(extracted_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c35df41-ac97-4ba0-961e-39e76e611549",
   "metadata": {},
   "source": [
    "#### The next step is to ask for a natural language interpretation of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9311aae-7d37-46f7-846b-8389369adbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_sys_prompt = [{\n",
    "    \"text\": \"\"\"  \n",
    "You are an SQL expert who can understand the intent behind a given SQL query. \n",
    "Translate the SQL query into a natural language request that a real user might make. \n",
    "\n",
    "- Keep your translation concise and conversational, mimicking how an actual user would ask for the information sought by the query. \n",
    "- Do not reference the <description> section directly and do not use a question form. \n",
    "- Ensure to include all conditions specified in the SQL query in the request.\n",
    "- Write possible business and functional purposes of the query.\n",
    "- Write very detailed purposes and motives of the query in detail.\n",
    "- Skip the preamble and phrase only the natural language request using a concise and straightforward tone without a verb ending. \n",
    "\"\"\"\n",
    "}]\n",
    "\n",
    "\n",
    "def get_translation_prompt(description, sql):\n",
    "    return [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": f\"<description>{description}<description>\\n\\n SQL: {sql}\"}]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733a116-4d58-4543-86fe-d34858c8199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = \"us-west-2\"\n",
    "llm_model = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "\n",
    "response = converse_with_bedrock(boto3_client, translation_sys_prompt, get_translation_prompt(extracted_description, queries[8]))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d725d-dcda-4be5-8ffe-130ad299bb7c",
   "metadata": {},
   "source": [
    "#### The natural language description for the following query was defined by the LLM as shown above.\n",
    "\n",
    "`SELECT CustomerId, SUM(Total) AS TotalPurchase FROM Invoice GROUP BY CustomerId ORDER BY TotalPurchase DESC LIMIT 5`\n",
    "\n",
    "#### Below is a script that repeats the above process for all SQL queries. (Takes about 1-2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7a078-d993-4afd-b460-45a0806dcf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "FILE_PATH_1 = './example_queries_temp.jsonl'\n",
    "def query_translation(table_info, queries):\n",
    "    if os.path.exists(FILE_PATH_1):\n",
    "        os.remove(FILE_PATH_1)\n",
    "\n",
    "    with open(FILE_PATH_1, 'a') as output_file:\n",
    "        for query in queries:\n",
    "            sql = query.strip()\n",
    "            \n",
    "            try:\n",
    "                response = converse_with_bedrock(boto3_client, extraction_sys_prompt, get_extraction_prompt(sql))\n",
    "                schema = json.loads(response)\n",
    "            except json.JSONDecodeError:\n",
    "                print(response)\n",
    "\n",
    "            description = extract_descriptions(table_info, schema[\"table\"], schema[\"column\"])\n",
    "\n",
    "            input = converse_with_bedrock(boto3_client, translation_sys_prompt, get_translation_prompt(description, sql))\n",
    "            \n",
    "            # Write input and query to the file in JSON format\n",
    "            data = {\"input\": input, \"query\": sql}\n",
    "            output_file.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "query_translation(schema_description, queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaef577-90ef-41e9-89ad-49c4835a2685",
   "metadata": {},
   "source": [
    "The results of the completed query transformations are stored in the `./lab3_text2sql_schema_preparation/example_queries_temp.jsonl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661cc2ab-3114-444c-bbec-b0f8fcc6fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_PATH_1, 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc567324-1948-4c8d-ad41-d48a07473a8a",
   "metadata": {},
   "source": [
    "## Step 3: Sample Query Vector Embedding and OpenSearch Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac40c19-7081-454f-8087-a364b66ee828",
   "metadata": {},
   "source": [
    "We need to embed the natural language questions from the <natural language question & SQL query> combinations into vectors. \n",
    "\n",
    "This is to facilitate finding SQL queries similar to user questions. The following code initializes the OpenSearch environment. (Creating connections and initializing Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ea722-2763-4ab1-b2f1-a90e1e7b371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "\n",
    "INDEX_NAME = \"example_queries\"\n",
    "\n",
    "def load_opensearch_config():\n",
    "    with open(\"./libs/opensearch.yml\", 'r', encoding='utf-8') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def init_opensearch(config):\n",
    "    mapping = {\"settings\": config['settings'], \"mappings\": config['mappings-sql']}\n",
    "    endpoint = opensearch_domain_endpoint\n",
    "    http_auth = (opensearch_user_id, opensearch_user_password)\n",
    "\n",
    "    os_client = OpenSearch(\n",
    "            hosts=[{'host': endpoint.replace(\"https://\", \"\"),'port': 443}],\n",
    "            http_auth=http_auth, \n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            timeout=300,\n",
    "            connection_class=RequestsHttpConnection\n",
    "    )\n",
    "\n",
    "    create_os_index(os_client, mapping)\n",
    "    return os_client\n",
    "\n",
    "def create_os_index(os_client, mapping):\n",
    "    exists = os_client.indices.exists(INDEX_NAME)\n",
    "\n",
    "    if exists:\n",
    "        os_client.indices.delete(index=INDEX_NAME)\n",
    "        print(\"Existing index has been deleted. Create new one.\")\n",
    "    else:\n",
    "        print(\"Index does not exist, Create one.\")\n",
    "\n",
    "    os_client.indices.create(INDEX_NAME, body=mapping)\n",
    "\n",
    "config = load_opensearch_config()\n",
    "os_client = init_opensearch(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea2db1-648b-4fd5-9186-9fc8abc6a20e",
   "metadata": {},
   "source": [
    "We will convert the previously created <natural language question & SQL query> pairs into vector embeddings, and format them into a Data-Action format suitable for bulk indexing in OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd7dd3-ad79-4953-ada5-bcc0e1c46026",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_2 = './example_queries.jsonl'\n",
    "\n",
    "embed_model = \"amazon.titan-embed-text-v2:0\"\n",
    "region_name = \"us-west-2\"\n",
    "\n",
    "def input_embedding():\n",
    "    num = 0\n",
    "    if os.path.exists(FILE_PATH_2):\n",
    "        os.remove(FILE_PATH_2)\n",
    "\n",
    "    with open(FILE_PATH_1, 'r') as input_file, open(FILE_PATH_2, 'a') as output_file:\n",
    "        for line in input_file:\n",
    "            \n",
    "            data = json.loads(line)\n",
    "            input = data['input']\n",
    "            query = data['query']\n",
    "\n",
    "            response = boto3_client.invoke_model(\n",
    "                modelId=embed_model,\n",
    "                body=json.dumps({\"inputText\": input})\n",
    "            )\n",
    "\n",
    "            # Data part\n",
    "            body = { \"input\": input, \"query\": query, \"input_v\": json.loads(response['body'].read())['embedding'] }\n",
    "\n",
    "            # Action part\n",
    "            action = { \"index\": { \"_index\": INDEX_NAME, \"_id\": str(num) } }\n",
    "\n",
    "            # Write action and body to the file in correct bulk format\n",
    "            output_file.write(json.dumps(action, ensure_ascii=False) + \"\\n\")\n",
    "            output_file.write(json.dumps(body, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            num += 1    \n",
    "\n",
    "input_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda2f81-7c57-4333-a095-3304ad1dabe9",
   "metadata": {},
   "source": [
    "In the `./lab3_text2sql_schema_preparation/example_queries.jsonl` file, you can see the converted embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d151126f-16f1-45f5-9b11-ade378977981",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILE_PATH_2, 'r') as file:\n",
    "    bulk_data = file.read()\n",
    "        \n",
    "response = os_client.bulk(body=bulk_data)\n",
    "if response[\"errors\"]:\n",
    "    print(\"There were errors during bulk indexing:\")\n",
    "    for item in response[\"items\"]:\n",
    "        if 'index' in item and item['index']['status'] >= 400:\n",
    "            print(f\"Error: {item['index']['error']['reason']}\")\n",
    "else:\n",
    "    print(\"Bulk-inserted all items successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b51586-09a7-4e02-8cb1-aae1ae0d9f96",
   "metadata": {},
   "source": [
    "#### Now, the sample queries have been stored into OpenSearch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
