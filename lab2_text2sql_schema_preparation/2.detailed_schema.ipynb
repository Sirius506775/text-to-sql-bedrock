{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde141b9-c4c6-44b0-b939-2a8dca3b27fb",
   "metadata": {},
   "source": [
    "# Lab. 3-1 Schema Preparation-2\n",
    "\n",
    "In this notebook, we'll be focusing on the '3. Table Summarizer' process as illustrated in the diagram below.\n",
    "\n",
    "Typically, the Schema Linking process for multi-table structure is divided into two steps: table selection followed by column selection. It's crucial to have comprehensive descriptions for each table because if the wrong table is selected, all subsequent steps become meaningless.\n",
    "\n",
    "This notebook will simulate the process of using a LLM to create detailed descriptive documents for each table.\n",
    "\n",
    "![Intro](../images/text2sql/schema-prep-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d4222-ceff-47ea-b491-8adac0f315d8",
   "metadata": {},
   "source": [
    "## Step 0: OpenSearch Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c70963-24c8-4a71-8529-37ed362b5f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.ssm import parameter_store\n",
    "\n",
    "pm = parameter_store('us-west-2')\n",
    "domain_endpoint = pm.get_params(key=\"chatbot-opensearch_domain_endpoint\", enc=False)\n",
    "opensearch_domain_endpoint = f\"https://{domain_endpoint}\"\n",
    "opensearch_user_id = pm.get_params(key=\"chatbot-opensearch_user_id\", enc=False)\n",
    "opensearch_user_password = pm.get_params(key=\"chatbot-opensearch_user_password\", enc=True)\n",
    "print(opensearch_domain_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed1666-159c-42f4-9664-84161e1fdbd9",
   "metadata": {},
   "source": [
    "## Step 1: Loading `Schema Description` & `Example Queries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6749a59-1105-49ce-9601-1bdb91d7fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "SCHEMA_FILE_PATH = \"./chinook_schema.json\"\n",
    "SAMPLE_QUERY_FILE_PATH = \"./example_queries_temp.jsonl\"\n",
    "\n",
    "def load_schema(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        schema = json.load(file)\n",
    "    return schema\n",
    "\n",
    "def load_queries(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        queries = file.readlines()\n",
    "    return queries\n",
    "\n",
    "schema = load_schema(SCHEMA_FILE_PATH)\n",
    "queries = load_queries(SAMPLE_QUERY_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a650d-2612-440c-835a-ebd1dbab66a4",
   "metadata": {},
   "source": [
    "## Step 2: Table summrization\n",
    "\n",
    "We utilize various information to generate table summary documents.\n",
    "\n",
    "We create table summaries using all available resources, including the basic Schema Description document and Sample Queries.\n",
    "\n",
    "Below is an LLM prompt template designed to incorporate this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9b880-aba2-4708-8f83-1287771a3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_sys_prompt = [{\n",
    "    \"text\": \"\"\" \n",
    "You are a data analyst. Summarize the provided SQL table based on the given context.\n",
    "\n",
    "<instruction> \n",
    "- Write a detailed summary based solely on the provided information. \n",
    "- Focus on the table's contents and structure, not on sample queries. \n",
    "- Describe the data types and content objectively, without subjective adjectives. \n",
    "- Do not mention sample queries or speculate on who might use the table. \n",
    "- Include potential use cases: questions the table can answer and analyses it enables. \n",
    "- Provide a concise summary without any preamble or introduction. \n",
    "</instruction>\n",
    "\"\"\" \n",
    "}]\n",
    "\n",
    "def get_summarization_prompt(table_schema, sample_queries):\n",
    "    return [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": f\"\"\"<table schema>\n",
    "{table_schema}\n",
    "</table schema>\n",
    "\n",
    "<sample queries>\n",
    "{sample_queries}\n",
    "</sample queries>\"\"\"}]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb10ee72-7793-4985-a4ee-aeda73e1d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "region_name = \"us-west-2\"\n",
    "llm_model = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "\n",
    "def init_boto3_client(region: str):\n",
    "    retry_config = Config(\n",
    "        region_name=region,\n",
    "        retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    "    )\n",
    "    return boto3.client(\"bedrock-runtime\", region_name=region, config=retry_config)\n",
    "\n",
    "def converse_with_bedrock(boto3_client, sys_prompt, usr_prompt):    \n",
    "    temperature = 0.0\n",
    "    top_p = 0.1\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "    \n",
    "    response = boto3_client.converse(\n",
    "        modelId=llm_model, \n",
    "        messages=usr_prompt, \n",
    "        system=sys_prompt,\n",
    "        inferenceConfig=inference_config\n",
    "    )\n",
    "\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "def search_table_queries(queries, table_name): \n",
    "    table_name_lower = table_name.lower()\n",
    "    matched_queries = []\n",
    "\n",
    "    for line in queries:\n",
    "        try:\n",
    "            query_data = json.loads(line)\n",
    "            if table_name_lower in query_data['query'].lower():\n",
    "                matched_queries.append(query_data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Invalid JSON line: {line}\")\n",
    "    \n",
    "    return matched_queries\n",
    "\n",
    "boto3_client = init_boto3_client(region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8583e3f3-f34a-4854-a580-27644c92f25e",
   "metadata": {},
   "source": [
    "#### Based on the given information, we will extract a summary document for the table named `Customer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3719069d-dc8c-4de1-b289-746c83fb619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'Customer'\n",
    "\n",
    "matched_queries = search_table_queries(queries, table_name)\n",
    "print(\"matched queries:\\n\", matched_queries, \"\\n\")\n",
    "\n",
    "table_summary = converse_with_bedrock(boto3_client, \n",
    "                                      summarization_sys_prompt, \n",
    "                                      get_summarization_prompt(schema[0][table_name], matched_queries))\n",
    "\n",
    "print(table_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f811120-5c0d-4478-81f1-f66469dbb093",
   "metadata": {},
   "source": [
    "#### The code below performs this operation for all tables in the Schema Description (it takes about 2-3 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72ab1fd-3a57-4aca-9070-e7f0530e5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_FILE_PATH1 = \"./chinook_detailed_schema_temp.json\"\n",
    "\n",
    "def summarize_table(table_name, table_desc, summarization_sys_prompt, queries):\n",
    "    table_summary = converse_with_bedrock(boto3_client, \n",
    "                                          summarization_sys_prompt, \n",
    "                                          get_summarization_prompt(table_desc, queries))\n",
    "    table_desc['table_summary'] = table_summary \n",
    "    return {table_name: table_desc}\n",
    "\n",
    "def write_summaries_to_file(summaries, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(summaries, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def process_schema(schema, summarization_sys_prompt, queries):\n",
    "    summaries = []\n",
    "    for table_info in schema:\n",
    "        for table_name, table_desc in table_info.items():\n",
    "            matched_queries = search_table_queries(queries, table_name)\n",
    "            summary = summarize_table(table_name, table_desc, summarization_sys_prompt, matched_queries)\n",
    "            summaries.append(summary)\n",
    "    return summaries\n",
    "\n",
    "table_summaries = process_schema(schema, summarization_sys_prompt, queries)\n",
    "write_summaries_to_file(table_summaries, OUTPUT_FILE_PATH1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556af33c-7a19-4df4-b3fa-d7fa38836eb7",
   "metadata": {},
   "source": [
    "In the `chinook_detailed_schema_temp.json` file, you'll see that the table_summary has been added to the schema document.\n",
    "\n",
    "As demonstrated above, providing the LLM with detailed information about 1) what columns are in the table, and 2) how the table is used, helps in selecting the correct table.\n",
    "\n",
    "However, when the table summaries become too long, it's not feasible to pass summaries of all tables to the LLM. In such cases, it's better to explore the table summary information using vector similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e56a62d-12da-4548-ac46-49c322dbbd99",
   "metadata": {},
   "source": [
    "## Step 3: Transform documents to vector embeddings and Store in OpenSearch\n",
    "\n",
    "This step proceeds similarly to the sample query storage process performed in `1.sample_queries.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fbb15a-f521-482e-b5df-6a8cd8d4ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "INDEX_NAME = \"schema_description\"\n",
    "\n",
    "def load_opensearch_config():\n",
    "    with open(\"../libs/opensearch.yml\", 'r', encoding='utf-8') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def init_opensearch(config):\n",
    "    mapping = {\"settings\": config['settings'], \"mappings\": config['mappings-detailed-schema']}\n",
    "    endpoint = opensearch_domain_endpoint\n",
    "    http_auth = (opensearch_user_id, opensearch_user_password)\n",
    "\n",
    "    os_client = OpenSearch(\n",
    "            hosts=[{'host': endpoint.replace(\"https://\", \"\"),'port': 443}],\n",
    "            http_auth=http_auth, \n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            timeout=300,\n",
    "            connection_class=RequestsHttpConnection\n",
    "    )\n",
    "\n",
    "    create_os_index(os_client, mapping)\n",
    "    return os_client\n",
    "\n",
    "def create_os_index(os_client, mapping):\n",
    "    exists = os_client.indices.exists(INDEX_NAME)\n",
    "\n",
    "    if exists:\n",
    "        os_client.indices.delete(index=INDEX_NAME)\n",
    "        print(\"Existing index has been deleted. Create new one.\")\n",
    "    else:\n",
    "        print(\"Index does not exist, Create one.\")\n",
    "\n",
    "    os_client.indices.create(INDEX_NAME, body=mapping)\n",
    "\n",
    "config = load_opensearch_config()\n",
    "os_client = init_opensearch(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827bcc97-974a-4fa2-b643-ca6f7519e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = \"amazon.titan-embed-text-v2:0\"\n",
    "region_name = \"us-west-2\"\n",
    "\n",
    "OUTPUT_FILE_PATH2 = \"./chinook_detailed_schema.json\"\n",
    "\n",
    "def summary_embedding():\n",
    "    with open(OUTPUT_FILE_PATH1, 'r', encoding='utf-8') as input_file:\n",
    "        data_list = json.load(input_file)\n",
    "\n",
    "    for data in data_list:\n",
    "        table_name = list(data.keys())[0]\n",
    "        table_summary = data[table_name][\"table_summary\"]\n",
    "\n",
    "        response = boto3_client.invoke_model(\n",
    "                modelId=embed_model,\n",
    "                body=json.dumps({\"inputText\": table_summary})\n",
    "            )\n",
    "        \n",
    "        data[table_name][\"table_summary_v\"] = json.loads(response['body'].read())['embedding']\n",
    "    \n",
    "    with open(OUTPUT_FILE_PATH2, 'w', encoding='utf-8') as output_file:\n",
    "        json.dump(data_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "summary_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30edc234-6e09-48df-9a21-491683c00666",
   "metadata": {},
   "source": [
    "If you now open the `chinook_detailed_schema_temp.json` file,\n",
    "you'll see that the table_summary and its corresponding embedding have been added to the schema document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ea3a0-7f48-441c-9d73-faa6f3fd472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_detailed_schema_descriptions(os_client):\n",
    "\n",
    "    with open(OUTPUT_FILE_PATH2, 'r') as file:\n",
    "        schema_data = json.load(file)\n",
    "\n",
    "    bulk_data = []\n",
    "    for table in schema_data:\n",
    "        for table_name, table_info in table.items():\n",
    "            table_doc = {\n",
    "                \"table_name\": table_name,\n",
    "                \"table_desc\": table_info[\"table_desc\"],\n",
    "                \"columns\": [{\"col_name\": col[\"col\"], \"col_desc\": col[\"col_desc\"]} for col in table_info[\"cols\"]],\n",
    "                \"table_summary\": table_info[\"table_summary\"],\n",
    "                \"table_summary_v\": table_info[\"table_summary_v\"]\n",
    "            }\n",
    "            bulk_data.append({\"index\": {\"_index\": INDEX_NAME, \"_id\": table_name}})\n",
    "            bulk_data.append(table_doc)\n",
    "    \n",
    "    bulk_data_str = '\\n'.join(json.dumps(item) for item in bulk_data) + '\\n'\n",
    "\n",
    "    response = os_client.bulk(body=bulk_data_str)\n",
    "    if response[\"errors\"]:\n",
    "        print(\"There were errors during bulk indexing:\")\n",
    "        for item in response[\"items\"]:\n",
    "            if 'index' in item and item['index']['status'] >= 400:\n",
    "                print(f\"Error: {item['index']['error']['reason']}\")\n",
    "    else:\n",
    "        print(\"Bulk-inserted all items successfully.\")\n",
    "\n",
    "load_detailed_schema_descriptions(os_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3bae4d-72fb-4086-a708-1195335c8fd2",
   "metadata": {},
   "source": [
    "#### Now, the schema description has been stored into OpenSearch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
